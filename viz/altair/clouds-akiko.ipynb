{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e824f402",
   "metadata": {},
   "source": [
    "# Word Cloud Prototypes | Team Enron Vizards\n",
    "\n",
    "### Course: W209 - Section 5\n",
    "### Author: Akiko Iwamizu\n",
    "\n",
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c9d90c",
   "metadata": {},
   "source": [
    "## About The Data\n",
    "\n",
    "Provided by [Kaggle Link](https://www.kaggle.com/datasets/wcukierski/enron-email-dataset), the Enron email dataset contains approximately 500,000 emails generated by employees of the Enron Corporation. It was obtained by the Federal Energy Regulatory Commission during its investigation of Enron's collapse.\n",
    "\n",
    "We conducted basically data preparation and manipulation before beginning EDA. These steps are not included in this notebook, but can be found in our team repo in [GitHub](https://github.com/akikoiwamizu/enron-vizards)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dee900",
   "metadata": {},
   "source": [
    "We begin the exploratory data analysis by importing the required libraries and the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b2d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "from ast import literal_eval\n",
    "from matplotlib import pyplot as plt\n",
    "from ipywidgets import widgets, interact, interactive, fixed\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer as stemmer\n",
    "from nltk.stem import WordNetLemmatizer as lemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"mode.chained_assignment\", None)\n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "nltk.download(\"punkt\") # necessary for tokenization\n",
    "nltk.download(\"wordnet\") # necessary for lemmatization\n",
    "nltk.download(\"stopwords\") # necessary for removal of stop words\n",
    "nltk.download(\"averaged_perceptron_tagger\") # necessary for POS tagging\n",
    "nltk.download(\"maxent_ne_chunker\" ) # necessary for entity extraction\n",
    "nltk.download(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa70b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"../../data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b346f210",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/emails_clean.csv\", \n",
    "                 converters=\n",
    "                 {\n",
    "                     \"X-From\": literal_eval,\n",
    "                     \"X-To\": literal_eval,\n",
    "                     \"X-cc\": literal_eval,\n",
    "                     \"X-bcc\": literal_eval\n",
    "                 }\n",
    ")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc991c8a",
   "metadata": {},
   "source": [
    "After loading in the cleaned dataset, let's take a look at the initial dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b9d25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd592f6",
   "metadata": {},
   "source": [
    "### Adding useful date dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67df125",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create date/time fields that we will need for this viz\n",
    "df[\"hour\"] = pd.to_datetime(df[\"time\"]).dt.hour.values\n",
    "df[\"date_clean\"] = pd.to_datetime(df[[\"year\", \"month\", \"day\"]])\n",
    "df[\"weekday\"] = pd.to_datetime(df[\"date_clean\"]).dt.day_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410834c6",
   "metadata": {},
   "source": [
    "### Standardize names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb662f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.copy()\n",
    "\n",
    "# Email authors\n",
    "authors = pd.Series([item for row in df_clean[\"X-From\"] for item in row if item])\n",
    "authors = authors.apply(lambda x: x.split(\"@\")[0].replace(\"\\\"\", \"\").replace(\"'\", \"\").replace(\".\", \" \").replace(\"_\", \" \").strip().title())\n",
    "authors = pd.DataFrame({\"name\": authors.value_counts().index,\n",
    "                        \"sent\": authors.value_counts()}).reset_index(drop=True).sort_values(by=\"sent\", ascending=False)\n",
    "\n",
    "# Email recipients\n",
    "all_recipients = df_clean[\"X-To\"] + df_clean[\"X-cc\"] + df_clean[\"X-bcc\"]\n",
    "recipients = pd.Series([item for row in all_recipients for item in row if item])\n",
    "recipients = recipients.apply(lambda x: x.split(\"@\")[0].replace(\"\\\"\", \"\").replace(\"'\", \"\").replace(\".\", \" \").replace(\"_\", \" \").strip().title())\n",
    "recipients = pd.DataFrame({\"name\": recipients.value_counts().index,\n",
    "                           \"received\": recipients.value_counts()}).reset_index(drop=True).sort_values(by=\"received\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1920035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize all name variations for the key players in the scandal\n",
    "andrew_fastow = authors.name[authors.name.apply(lambda x: \"Andrew S Fastow\" in x)]\n",
    "arthur_andersen = authors.name[authors.name.apply(lambda x: \"Arthur Andersen\" in x)]\n",
    "jeff_skilling = authors.name[authors.name.apply(lambda x: \"Jeff Skilling\" in x)]\n",
    "ken_lay = authors.name[authors.name.apply(lambda x: \"Ken Lay\" in x)]\n",
    "sherron_watkins = authors.name[authors.name.apply(lambda x: \"Sherron Watkins\" in x)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133b1330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean dataset with correct names\n",
    "def clean_names(name_list):\n",
    "    for i, name in enumerate(name_list):\n",
    "        if name in andrew_fastow:\n",
    "            name_list[i] = \"Andrew Fastow\"\n",
    "        elif name in arthur_andersen:\n",
    "            name_list[i] = \"Arthur Andersen\"\n",
    "        elif name in jeff_skilling:\n",
    "            name_list[i] = \"Jeff Skilling\"\n",
    "        elif name in ken_lay:\n",
    "            name_list[i] = \"Kenneth Lay\"\n",
    "        elif name in sherron_watkins:\n",
    "            name_list[i] = \"Sherron Watkins\"\n",
    "        return name_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34868e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[\"X-From\"] = df_clean[\"X-From\"].apply(lambda x: clean_names(x))\n",
    "df_clean[\"X-To\"] = df_clean[\"X-To\"].apply(lambda x: clean_names(x)) \n",
    "df_clean[\"X-cc\"] = df_clean[\"X-cc\"].apply(lambda x: clean_names(x)) \n",
    "df_clean[\"X-bcc\"] = df_clean[\"X-bcc\"].apply(lambda x: clean_names(x))\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d752ee",
   "metadata": {},
   "source": [
    "### Create a data subset for only the scandal period (2001-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5febf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of the email dataset to only the scandal period\n",
    "# Defining the scandal period as the date when Bethany McLean published the article \"Is Enron Overpriced?\"\n",
    "# in Fortune magazine. She writes that investors are generally clueless as to how Enron earns its reported profits.\n",
    "scandal_period = df_clean[df_clean[\"date_clean\"] >= \"2001-09-30\"]\n",
    "scandal_period.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f9f09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scandal_period)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7e89c1",
   "metadata": {},
   "source": [
    "### Find the most common words in the email messages during the scandal period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3d9459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Email authors + their messages\n",
    "email_messages = scandal_period[[\"date_clean\", \"From\", \"X-From\", \"Body\"]].reset_index(drop=True)\n",
    "\n",
    "# Clean up \"From\" field\n",
    "email_messages[\"Name\"] = email_messages[\"X-From\"].str[0]\n",
    "email_messages[\"Name\"] = email_messages[\"Name\"].apply(lambda x: x.split(\"@\")[0].replace(\"\\\"\", \"\").replace(\"'\", \"\").replace(\".\", \" \").replace(\"_\", \" \").strip().title())\n",
    "\n",
    "email_messages.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b937215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spacy\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def clean_string(text, stem=\"None\"):\n",
    "\n",
    "    final_string = \"\"\n",
    "\n",
    "    # Make lower\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove line breaks\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "\n",
    "    # Remove puncuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    # Remove stop words\n",
    "    text = text.split()\n",
    "    useless_words = nltk.corpus.stopwords.words(\"english\")\n",
    "    useless_words = useless_words + [\"im\", \"cc\", \"subject\", \"re\", \"na\", \"original\", \"message\", \"image\", \"email\", \"address\"]\n",
    "\n",
    "    text_filtered = [word for word in text if not word in useless_words]\n",
    "\n",
    "    # Remove numbers\n",
    "    text_filtered = [re.sub(r'\\w*\\d\\w*', '', w) for w in text_filtered]\n",
    "\n",
    "    # Stem or Lemmatize\n",
    "    if stem == 'Stem':\n",
    "        stemmer = PorterStemmer() \n",
    "        text_stemmed = [stemmer.stem(y) for y in text_filtered]\n",
    "    elif stem == 'Lem':\n",
    "        lem = WordNetLemmatizer()\n",
    "        text_stemmed = [lem.lemmatize(y) for y in text_filtered]\n",
    "    elif stem == 'Spacy':\n",
    "        text_filtered = nlp(' '.join(text_filtered))\n",
    "        text_stemmed = [y.lemma_ for y in text_filtered]\n",
    "    else:\n",
    "        text_stemmed = text_filtered\n",
    "\n",
    "    final_string = ' '.join(text_stemmed)\n",
    "\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7517b4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next apply the clean_string function to the text\n",
    "email_messages[\"Body_Clean\"] = email_messages[\"Body\"].apply(lambda x: clean_string(x, stem='Lem'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a276dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94e0cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export snapshot of data + compress\n",
    "compression_opts = dict(method='zip', archive_name='scandal_period2.csv')  \n",
    "email_messages.to_csv('scandal_period2.zip', index=False, compression=compression_opts)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a08bfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Remove stopwords\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"please\", \"hi\", \"im\", \"ill\", \"cc\", \"subject\", \"re\", \"na\", \"original\", \"message\", \"image\", \"email\", \"address\"])\n",
    "\n",
    "wordcloud = WordCloud(stopwords=stopwords,\n",
    "                      background_color=\"white\", \n",
    "                      max_words=100,\n",
    "                      width=1024, \n",
    "                      height=500,\n",
    "                      colormap=\"magma\"                      \n",
    "                     ).generate(text)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    "plt.show()\n",
    "plt.savefig(\"cloud_all.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39223ed3",
   "metadata": {},
   "source": [
    "### Create a subset of the email data to include only the key players of the scandal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83874d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Email authors + their messages\n",
    "key_players = df_clean[[\"date_clean\", \"From\", \"X-From\", \"X-To\", \"Body\"]].reset_index(drop=True)\n",
    "\n",
    "# Clean up \"From\" field\n",
    "key_players[\"Name_From\"] = key_players[\"X-From\"].str[0]\n",
    "key_players[\"Name_To\"] = key_players[\"X-To\"].str[0]\n",
    "key_players[\"Name_From\"] = key_players[\"Name_From\"].apply(lambda x: x.split(\"@\")[0].replace(\"\\\"\", \"\").replace(\"'\", \"\").replace(\".\", \" \").replace(\"_\", \" \").strip().title())\n",
    "key_players[\"Name_To\"] = key_players[\"Name_To\"].apply(lambda x: x.split(\"@\")[0].replace(\"\\\"\", \"\").replace(\"'\", \"\").replace(\".\", \" \").replace(\"_\", \" \").strip().title())\n",
    "key_players.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4938cbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of email messages from key players in the scandal\n",
    "key_names = [\"Andrew Fastow\", \"Jeff Skilling\", \"Kenneth Lay\", \"Sherron Watkins\"]\n",
    "key_players = key_players[[\"date_clean\", \"Name_From\", \"Name_To\", \"Body\"]]\n",
    "key_players = key_players.loc[key_players[\"Name_From\"].isin(key_names) | key_players[\"Name_To\"].isin(key_names)].reset_index(drop=True)\n",
    "key_players.drop_duplicates()\n",
    "len(key_players)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce46e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next apply the clean_string function to the text\n",
    "key_players[\"Body_Clean\"] = key_players[\"Body\"].apply(lambda x: clean_string(x, stem='Lem'))\n",
    "\n",
    "key_players = key_players[[\"date_clean\", \"Name_From\", \"Name_To\", \"Body_Clean\"]]\n",
    "key_players.drop_duplicates()\n",
    "len(key_players)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01304238",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "key_players.head(-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcd22d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = key_players[\"Body_Clean\"].str.split(expand=True).stack().value_counts().reset_index()\n",
    "test.columns = [\"Word\", \"Count\"]\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bac3071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export snapshot of data + compress\n",
    "compression_opts = dict(method='zip', archive_name='key_players.csv')  \n",
    "key_players.to_csv('key_players.zip', index=True, compression=compression_opts)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e0a08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "text = \" \".join(item for item in key_players[\"Body_Clean\"])\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"jeffskillinenroncom\", \"u\", \"skillingcorpenronenron\", \"please\", \"hi\", \"im\", \"ill\", \"cc\", \"subject\", \"re\", \"na\", \"original\", \"message\", \"image\", \"email\", \"address\"])\n",
    "\n",
    "wordcloud = WordCloud(stopwords=stopwords,\n",
    "                      background_color=\"white\", \n",
    "                      max_words=100,\n",
    "                      width=1024, \n",
    "                      height=500,\n",
    "                      colormap=\"magma\"                      \n",
    "                     ).generate(text)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    "plt.show()\n",
    "plt.savefig(\"cloud_key_players.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9079b0d",
   "metadata": {},
   "source": [
    "To improve the visualization, let's allow users to interactively select the key players in the scandal and see how their word clouds change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6519b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "def clean_emails(Employee, frame, col):\n",
    "    newframe=frame.copy()  \n",
    "    newframe[col] = newframe[col].str.replace('\\d+', \"\", regex=True).str.replace(\"?\", \"\", regex=True).str.replace('\\W', \" \", regex=True).str.lower()\n",
    "    newframe = newframe[newframe[\"Name\"]==Employee].astype(str)\n",
    "    \n",
    "    text = \" \".join(newframe[col][:])\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(set(STOPWORDS)) + r')\\b\\s*')\n",
    "    cleantext = pattern.sub(\"\", text)\n",
    "    \n",
    "    text_tokenized = cleantext.split(\" \")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in text_tokenized if word != \"\"]\n",
    "    cloudtext = \" \".join(words)\n",
    "    \n",
    "    return cloudtext\n",
    "\n",
    "def make_clouds(Employee, frame, col, Maximum, title):\n",
    "    cloudtext = clean_emails(Employee, frame, col)\n",
    "    wordcloud = WordCloud(max_font_size=40, max_words=Maximum, background_color=\"white\", collocations=False).generate(cloudtext)\n",
    "    wordcloud.generate_from_frequencies\n",
    "    \n",
    "    plt.style.use(\"tableau-colorblind10\")\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(title, fontsize=18, fontweight=\"bold\")\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "cloud = interact(make_clouds, \n",
    "         Employee=key_players,\n",
    "         Maximum=[100, 75, 50, 25],\n",
    "         df=fixed(email_messages_kp), \n",
    "         col=fixed(\"Body\"), \n",
    "         title=fixed(\"\\nMost Common Words in Emails by the Enron Scandal Key Players\\n\"),\n",
    "         frame=fixed(email_messages_kp[[\"Name\",\"Body\"]])\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bebd4f",
   "metadata": {},
   "source": [
    "#### Conclusion: \n",
    "\n",
    "These data views suggest that the most common words used by key players in their emails vary quite a bit depending on the employee. Across all the key players, there are various greetings and meeting times like the key player's names and \"Monday\" or \"pm\" likely for scheduling purposes, but there are also many common words like \"enronxgate\", \"investigation\", and various references to internal initiatives like the petrobas gas initiative. \n",
    "\n",
    "---------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
